# Log Analytics System for a Growing Enterprise

This project simulates a real-world enterprise log analytics system using Hadoop.

## Project Overview
The Log Analytics System for a Growing Enterprise is a Hadoop-based solution designed to process and analyze increasing volumes of web access logs generated by internal applications and administrative systems. Each user interaction produces structured log entries containing IP addresses, timestamps, HTTP methods, URIs, status codes, and user-agent information.

As log volumes grow daily, the project evaluates Hadoop’s capabilities in scalability, fault tolerance, and performance before enterprise-wide adoption. 

## Tech Stack
```
Operating System: Linux/ WSL (Windows)
Platform: Apache Hadoop
Storage: HDFS
Environment: Java JDK 8 or 11
Programming Language: Python (for Hadoop Streaming)
```

## Project Structure
```
enterprise-log-analyzer-hadoop /
|-- hadoop_setup /
|   |-- setup.sh
|   |-- setup_instructions.txt
|   |-- start.sh
|   |-- stop.sh
|
|-- source /
|   |-- built_in_word_count.sh
|   |-- hadoop_evolution_study.txt
|   |-- large_log_ingestion.sh
|   |-- log_ingestion.sh
|
|-- word_and_log_hadoop_streaming /
|   |-- word_count.sh
|   |-- word_mapper.py
|   |-- word_reducer.py
|
|-- .gitignore
|-- README.md
|-- requirements.txt
```

## Installation & Setup
1. Clone the repository
bash
git clone https://github.com/Madhumitha212/hadoop-log-analytics-system.git

2. Install dependencies
bash
pip install -r requirements.txt

## Execution Flow (Phased Tasks)
Phase 1: Hadoop Environment Setup
Run ./hadoop_setup/setup.sh → install Hadoop and configure environment

Run ./hadoop_setup/start.sh → start Hadoop daemons

Validate HDFS read/write and check Web UIs

Phase 2: Log Ingestion & HDFS Block Analysis
Upload small log file → ./source/log_ingestion.sh

Analyze block allocation for small files

Upload large log file → ./source/large_log_ingestion.sh

Verify block splitting with 128 MB block size

Phase 3: Built-in MapReduce
Execute Hadoop’s WordCount → ./source/built_in_word_count.sh

Observe mapper count, reducer shuffle, and block-to-task mapping

Phase 4: Custom Python MapReduce (Streaming)
Run Python WordCount → ./word_and_log_hadoop_streaming/word_count.sh

Compare performance vs built-in WordCount

Phase 5: Error Log Analysis
Extend streaming jobs to filter HTTP status ≥ 400

Output frequency of error codes and endpoints

Phase 6: Hadoop Architecture Evolution Study
Documentation: source/hadoop_evolution_study.txt

Compare Hadoop 1.x, 2.x, and 3.x (JobTracker, YARN, HA improvements)

Phase 7: Configuration & Performance Tuning
Study Hadoop configs for HDFS, MapReduce, YARN

Recommend tuning changes for scalability and resource efficiency

Phase 8: Stop Services
Run ./hadoop_setup/stop.sh → stop Hadoop daemons

## Documentation
Setup Instructions → hadoop_setup/setup_instructions.txt
Hadoop Evolution Study → source/hadoop_evolution_study.txt

## Author
R Madhumitha 
