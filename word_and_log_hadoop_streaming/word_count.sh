#!/bin/bash

# Python WordCount using Hadoop Streaming

HADOOP_STREAMING_JAR=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar

INPUT_DIR=/logs/source/logfiles.log
OUTPUT_DIR=logs/python_wordcount_output

echo "===== Starting Python WordCount Job ====="

echo "Cleaning old output directory if it exists..."
hdfs dfs -rm -r $OUTPUT_DIR 2>/dev/null

echo "Submitting Hadoop Streaming job..."

hadoop jar $HADOOP_STREAMING_JAR \
  -input $INPUT_DIR \
  -output $OUTPUT_DIR \
  -mapper word_mapper.py \
  -reducer word_reducer.py \
  -file word_mapper.py \
  -file word_reducer.py

if [ $? -ne 0 ]; then
  echo "❌ Hadoop Streaming Job FAILED"
  exit 1
fi

echo "===== WordCount Output (Sample) ====="
hdfs dfs -cat $OUTPUT_DIR/part-00000 | head -20

echo "===== Hadoop Streaming Job Completed Successfully ====="

#Identify the number of mapper tasks launched
#The number of mapper tasks launched is determined by the number of input splits created by Hadoop.
#The number of input split is created by the number of blocks created by hadoop 
#Multiple mapper attempts may occur due to failures.

#Explain the relationship between input blocks and mapper count
#The number of mapper tasks launched is determined by the number of input splits created by Hadoop.
#Number of mappers ≈ Number of input splits ≈ Number of HDFS blocks

#Observe reducer execution and shuffle behavior
#During reducer execution, Hadoop performs the shuffle and sort phase, where intermediate key-value pairs generated by mappers are transferred to reducers.
