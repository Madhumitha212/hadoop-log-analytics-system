1. IntroductionPurpose: 	This document records the stepbystep installation and configuration of WSL (Windows Subsystem for Linux), Ubuntu 22.04, Java 8, and Hadoop. The goal is to create a reproducible setup guide for learning, review, and GitHub portfolio documentation.2. Install WSLCommand (run in Administrator PowerShell):C:Users\sandh>wsl --install -d Ubuntu-22.04Output:Installing: Ubuntu-22.04Ubuntu-22.04 has been installed.Launching Ubuntu-22.04...The requested operation is successful.Changes will not be effective until the system is rebooted.3. First Launch of UbuntuPurpose: Create a UNIX user account for your Linux environment.Output:Installing, this may take a few minutes...Please create a default UNIX user account.Enter new UNIX username: %%%%%%%%%%New password:%%%%%%%Retype new password:%%%%%%%Installation successful!4. Update & Upgrade PackagesPurpose: Ensure Ubuntu has the latest packages.Command:sudo apt update && sudo apt upgrade -yOutput:Hit:1 http://archive.ubuntu.com/ubuntu jammy InReleaseGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]Fetched 229 kB in 2s (115 kB/s)Reading package lists... DoneBuilding dependency treeReading state information... DoneCalculating upgrade... Done3 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.Setting up libc6 (2.35-0ubuntu3.4) ...Processing triggers for libc-bin (2.35-0ubuntu3.4) ...5. Install Java 8Purpose: Provide Hadoops runtime environment.Command:sudo apt install openjdk-8-jdk -yVerify Installation:java -versionOutput:openjdk version "1.8.0_352"OpenJDK Runtime Environment (build 1.8.0_352-b08)OpenJDK 64-Bit Server VM (build 25.352-b08, mixed mode)6. Download HadoopPurpose: Get Hadoop binaries from Apache repository.Command:cd ~wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gzls ~Output:--2026-02-05 14:45:00--  https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gzResolving downloads.apache.org... done.Connecting to downloads.apache.org... connected.HTTP request sent, awaiting response... 200 OKLength: 450 MB (hadoop-3.3.6.tar.gz)Saving to: hadoop-3.3.6.tar.gz7. Move Hadoop to D DrivePurpose: places Hadoop in a permanent location (D drive), extracts it, and prepares it for configuration.Command:mv ~/hadoop-3.3.6.tar.gz /mnt/d/cd /mnt/dtar -xvzf hadoop-3.3.6.tar.gzmv hadoop-3.3.6 hadoopsudo chown -R $USER:$USER /mnt/d/Hadoop8.Clean & Create Hadoop Data DirectoriesPurpose: Remove old data, create fresh directories, verify location, list contents.Command:sudo rm -rf /mnt/d/hadoop/datamkdir -p ~/hadoop-data/namenodemkdir -p ~/hadoop-data/datanodepwdlsoutput:Shows current directory: /home/%%%%%%hadoop-data9.Configure HadoopPurpose:  Defines the default filesystem (fs.defaultFS) that Hadoop will use. Setting it to hdfs://localhost:9000 tells Hadoop to connect to the local NameNode running on port 9000.Command:core-site.xmlnano /mnt/d/hadoop/etc/hadoop/core-site.xml<configuration>  <property>    <name>fs.defaultFS</name>    <value>hdfs://localhost:9000</value>  </property></configuration>Purpose:Configures HDFS storage behavior:dfs.replication=1 → Only one copy of each block (single-node setup).dfs.namenode.name.dir → Path for NameNode metadata storage.dfs.datanode.data.dir → Path for DataNode block storage.Command:hdfs-site.xmlnano /mnt/d/hadoop/etc/hadoop/hdfs-site.xml<configuration>  <property>    <name>dfs.replication</name>    <value>1</value>  </property>  <property>    <name>dfs.namenode.name.dir</name>    <value>file:/mnt/d/hadoop/data/namenode</value>  </property>  <property>    <name>dfs.datanode.data.dir</name>    <value>file:/mnt/d/hadoop/data/datanode</value>  </property></configuration>10. Set Hadoop Environment VariablesPurpose: Make Hadoop accessible from terminal.Command:nano ~/.bashrcAdd at the end:export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export HADOOP_HOME=/mnt/d/Hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinReload:source ~/.bashrc11.Format NameNodePurpose:creates the metadata structures required by the NameNode, such as:Block pool ID (unique identifier for your HDFS instance).Version files and storage directories for the NameNode.Command:hdfs namenode -format12.Install & Configure SSH (Required for Hadoop)Purpose:Hadoop daemons need to communicate with each other, SSH provides the mechanism for this communication.Command:sudo apt install -y openssh-serversudo service ssh startsudo service ssh statusOutput:Active: active (running) since Fri 2026-02-06 10:15:00 IST13.Enable Passwordless SSHPurpose: These commands configure SSH key‑based authentication so that Hadoop daemons (like NameNode and DataNode) can communicate without asking for a password every time. It’s essential for running scripts like start-dfs.sh and start-yarn.sh.Command:ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keyschmod 600 ~/.ssh/authorized_keysssh localhostexitOutput:Welcome to Ubuntu 22.04 LTS (GNU/Linux 5.15.0-xx-generic x86_64)14.Start HDFSPurpose: launches the Hadoop Distributed File System (HDFS) daemons.Command:start-dfs.shOutput:Starting namenodes on [localhost]Starting datanodesStarting secondary namenodes [0.0.0.0]15.Verify HadoopPurpose: The command jps (Java Virtual Machine Process Status Tool) is used to list all Java processes currently running on your system.Command:jpsOutput:NameNodeDataNodeSecondaryNameNodeJps16.Hadoop Web UI:Purpose:  Opening http://localhost:9870 in your browser lets you visually confirm that HDFS is running and explore its state without relying only on terminal commands.Command:http://localhost:9870Output(in browser at http://localhost:9870):Configured Capacity: 123.45 GBDFS Used: 0 GBNon DFS Used: 2 GBDFS Remaining: 121.45 GBLive Nodes: 1